{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "        self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "    \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        #print(out)\n",
    "        out = F.softmax(out, dim=0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "#import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import Tensor\n",
    "from scipy.io import wavfile\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawNetDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 transform=None,\n",
    "                 mode=\"train\",\n",
    "                 files_dir=None,\n",
    "                 base_dir=\"\",\n",
    "                 csv_file_dir=\"\",\n",
    "                 nb_time=59049,\n",
    "                 train=True):\n",
    "        \n",
    "        self.base_dir = base_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.csv_file_dir = csv_file_dir\n",
    "        self.files_dir = files_dir\n",
    "        csv_file = pd.read_csv(csv_file_dir,sep=\"\\t\")\n",
    "        self.nb_time = nb_time\n",
    "        self.train = train\n",
    "        #self.classes = {cls_name:i for i, cls_name in enumerate(csv_file[\"VoxCeleb1 ID\"].unique())}\n",
    "        self.classes = {'id10009': 0,'id10016': 1,'id10017': 2,'id10019': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.files_dir)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.files_dir[idx]\n",
    "        classe = filename.split('/')[0]\n",
    "        X, sample_rate = torchaudio.load(self.base_dir + filename)\n",
    "        #print(\" shape(X): \",type(X))\n",
    "        label = self.classes[classe]\n",
    "        self._pre_emphasis(X)\n",
    "        if self.train:\n",
    "            nb_time = X.shape[1]\n",
    "            if nb_time > self.nb_time:\n",
    "                start_idx = np.random.randint(low = 0,\n",
    "                    high = nb_time - self.nb_time)\n",
    "                X = X[:, start_idx:start_idx+self.nb_time]\n",
    "                #print(\"nb_time: \",nb_time )\n",
    "                #print(\"self.nb_time: \",self.nb_time)\n",
    "            elif nb_time < self.nb_time:\n",
    "                nb_dup = int(self.nb_time / nb_time) + 1\n",
    "                X = np.tile(X, (1, nb_dup))[:, :self.nb_time]\n",
    "                #print(\"taille inférieure\")\n",
    "            else:\n",
    "                X = X\n",
    "            #print(\"taille égale\")\n",
    "        #print(\" type(X): \",X.size())\n",
    "        return X, label\n",
    "\n",
    "    def _pre_emphasis(self, x):\n",
    "        '''\n",
    "        Pre-emphasis for single channel input\n",
    "        '''\n",
    "        return np.asarray(x[:,1:] - 0.97 * x[:, :-1], dtype=np.float32) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#input_size = 59049       # input size\n",
    "#hidden_size = 500      # The number of nodes at the hidden layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data/wav/'\n",
    "csv_file_dir = 'data/target/vox1_meta.csv'\n",
    "def get_utt_list(src_dir):\n",
    "    l_utt = []\n",
    "    for r, ds, fs in os.walk(src_dir):\n",
    "        r = r.replace('\\\\', '/')   \n",
    "        base = '/'.join(r.split('/')[-2:])+'/'\n",
    "        for f in fs:\n",
    "            l_utt.append(base+f[:-4]+'.wav')\n",
    "    return l_utt\n",
    "list_IDs = get_utt_list(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train,  validation data\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid = train_test_split(list_IDs, test_size=0.05, random_state=42, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN DATA\n",
    "num_classes = 4      # The number of output classes. In this case, from 0 to 3\n",
    "num_epochs = 5         # The number of times entire dataset is trained\n",
    "batch_size = 100       # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence\n",
    "train_dataset = RawNetDataset(files_dir=X_train,base_dir=base_dir,csv_file_dir=csv_file_dir, train=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATION DATA : for cross validation \n",
    "valid_dataset = RawNetDataset(files_dir=X_valid,base_dir=base_dir,csv_file_dir=csv_file_dir, train=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, \n",
    "                                          batch_size=len(valid_dataset), \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RawNet, self).__init__()\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.lrelu_keras = nn.LeakyReLU(negative_slope=0.3)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels = 1,#1\n",
    "\t\t\tout_channels = 128,#128\n",
    "\t\t\tkernel_size = 3,#3\n",
    "                        padding = 0,\n",
    "                        stride = 3\n",
    "        )\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(num_features = 128)\n",
    "        self.gru = nn.GRU(input_size = 256,\n",
    "\t\t\thidden_size = 1024,\n",
    "                        num_layers = 1,\n",
    "                        batch_first = True)\n",
    "\n",
    "        self.gru_fc1 = nn.Linear(in_features = 1024,\n",
    "                                 out_features = 1024)\n",
    "\n",
    "        self.gru_fc2 = nn.Linear(in_features = 1024,\n",
    "                                 out_features = 4)\n",
    "\n",
    "        self.bn_before_gru = nn.BatchNorm1d(num_features = 256)\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels = 128,\n",
    "                               out_channels = 128,\n",
    "                               kernel_size = 3,\n",
    "                               padding = 1,\n",
    "                               stride = 1\n",
    "        )\n",
    "\n",
    "\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=128)\n",
    "        \n",
    "        self.conv3_1_1 = nn.Conv1d(in_channels = 128,\n",
    "                               out_channels = 256,\n",
    "                               kernel_size = 3,\n",
    "                               padding = 1,\n",
    "                               stride = 1\n",
    "        )\n",
    "\n",
    "        self.conv3_1 = nn.Conv1d(in_channels = 256,\n",
    "                               out_channels = 256,\n",
    "                               kernel_size = 3,\n",
    "                               padding = 1,\n",
    "                               stride = 1\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        self.conv3_2 = nn.Conv1d(in_channels = 256,\n",
    "                               out_channels = 256,\n",
    "                               kernel_size = 3,\n",
    "                               padding = 1,\n",
    "                               stride = 1\n",
    "        )\n",
    "\n",
    "        self.conv3_3 = nn.Conv1d(in_channels = 128,\n",
    "                               out_channels = 256,\n",
    "                               kernel_size = 1,\n",
    "                               padding = 0,\n",
    "                               stride = 1\n",
    "        )\n",
    "\n",
    "\n",
    "        self.bn3_1_1 = nn.BatchNorm1d(num_features=256)\n",
    "        self.bn3_1 = nn.BatchNorm1d(num_features=128)\n",
    "        self.bn3_2 = nn.BatchNorm1d(num_features=256)\n",
    "\n",
    "\n",
    "        self.mp = nn.MaxPool1d(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.lrelu(out)\n",
    "\n",
    "        print(\"shape conv 1 : \", out.shape)\n",
    "\n",
    "        #-------- Block 1 --------------\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.lrelu_keras(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.mp(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        out = self.lrelu_keras(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.lrelu_keras(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.mp(out)\n",
    "\n",
    "        print(\"shape resblock 1 : \", out.shape)\n",
    "\n",
    "        #-------- Block 2 --------------\n",
    "        \n",
    "        out_identite = out\n",
    "        out = self.conv3_1_1(out)\n",
    "        out = self.bn3_2(out)\n",
    "        out = self.lrelu_keras(out)\n",
    "        out = self.conv3_2(out)\n",
    "        out += self.conv3_3(out_identite)\n",
    "        out = self.mp(out)\n",
    "\n",
    "        out_identite = out\n",
    "        out = self.bn3_1_1(out)\n",
    "        out = self.lrelu_keras(out)\n",
    "        out = self.conv3_1(out)\n",
    "        out = self.bn3_2(out)\n",
    "        out = self.lrelu_keras(out)\n",
    "        out = self.conv3_2(out)\n",
    "        out += out_identite\n",
    "        out = self.mp(out)\n",
    "\n",
    "        out_identite = out\n",
    "        out = self.bn3_1_1(out)\n",
    "        out = self.lrelu_keras(out)\n",
    "        out = self.conv3_1(out)\n",
    "        out = self.bn3_2(out)\n",
    "        out = self.lrelu_keras(out)\n",
    "        out = self.conv3_2(out)\n",
    "        out += out_identite\n",
    "        out = self.mp(out)\n",
    "\n",
    "        out_identite = out\n",
    "        out = self.bn3_1_1(out)\n",
    "        out = self.lrelu_keras(out)\n",
    "        out = self.conv3_1(out)\n",
    "        out = self.bn3_2(out)\n",
    "        out = self.lrelu_keras(out)\n",
    "        out = self.conv3_2(out)\n",
    "        out += out_identite\n",
    "        out = self.mp(out)\n",
    "\n",
    "        print(\"shape resblock 2 : \", out.shape)\n",
    "        \n",
    "\n",
    "        #-------- Gru --------------\n",
    "\n",
    "        out = self.bn_before_gru(out)\n",
    "        out = self.lrelu_keras(out)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        #(batch, filt, time) >> (batch, time, filt)\n",
    "\n",
    "        out, _ = self.gru(out)\n",
    "\n",
    "        out = out[:,-1,:]\n",
    "        code = self.gru_fc1(out)\n",
    "\n",
    "        code_norm = code.norm(p=2,dim=1, keepdim=True) / 10.\n",
    "        code = torch.div(code, code_norm)\n",
    "\n",
    "        print(\"shape GRU : \",code.shape)\n",
    "\n",
    "        out = self.gru_fc2(code)\n",
    "\n",
    "        print(\"shape output : \", out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.NLLLoss()\n",
    "rawnet = RawNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rawnet.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "def train_model():\n",
    "    rawnet.train()\n",
    "    for i, (data, labels) in enumerate(train_loader):   # Loadl a batch of audio with its (data, class)\n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = rawnet(data)                             # Forward pass: compute the output class given a audio\n",
    "        loss = criterion(outputs, labels)                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation \n",
    "def cross_validation(model, eval_loader):\n",
    "    model.eval()\n",
    "    for i, (data, labels) in enumerate(eval_loader):   # Loadl a batch of audio with its (data, class)\n",
    "        outputs = model(data)                             # Forward pass: compute the output class given a audio\n",
    "        loss = criterion(outputs, labels)                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        print(\"loss\", i, loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape conv 1 :  torch.Size([100, 128, 19683])\n",
      "shape resblock 1 :  torch.Size([100, 128, 2187])\n",
      "shape resblock 2 :  torch.Size([100, 256, 27])\n",
      "shape GRU :  torch.Size([100, 1024])\n",
      "shape output :  torch.Size([100, 4])\n",
      "shape conv 1 :  torch.Size([100, 128, 19683])\n",
      "shape resblock 1 :  torch.Size([100, 128, 2187])\n",
      "shape resblock 2 :  torch.Size([100, 256, 27])\n",
      "shape GRU :  torch.Size([100, 1024])\n",
      "shape output :  torch.Size([100, 4])\n",
      "shape conv 1 :  torch.Size([100, 128, 19683])\n",
      "shape resblock 1 :  torch.Size([100, 128, 2187])\n",
      "shape resblock 2 :  torch.Size([100, 256, 27])\n",
      "shape GRU :  torch.Size([100, 1024])\n",
      "shape output :  torch.Size([100, 4])\n",
      "shape conv 1 :  torch.Size([100, 128, 19683])\n",
      "shape resblock 1 :  torch.Size([100, 128, 2187])\n",
      "shape resblock 2 :  torch.Size([100, 256, 27])\n",
      "shape GRU :  torch.Size([100, 1024])\n",
      "shape output :  torch.Size([100, 4])\n",
      "shape conv 1 :  torch.Size([100, 128, 19683])\n",
      "shape resblock 1 :  torch.Size([100, 128, 2187])\n",
      "shape resblock 2 :  torch.Size([100, 256, 27])\n",
      "shape GRU :  torch.Size([100, 1024])\n",
      "shape output :  torch.Size([100, 4])\n",
      "shape conv 1 :  torch.Size([14, 128, 19683])\n",
      "shape resblock 1 :  torch.Size([14, 128, 2187])\n",
      "shape resblock 2 :  torch.Size([14, 256, 27])\n",
      "shape GRU :  torch.Size([14, 1024])\n",
      "shape output :  torch.Size([14, 4])\n",
      "shape conv 1 :  torch.Size([28, 128, 19683])\n",
      "shape resblock 1 :  torch.Size([28, 128, 2187])\n",
      "shape resblock 2 :  torch.Size([28, 256, 27])\n",
      "shape GRU :  torch.Size([28, 1024])\n",
      "shape output :  torch.Size([28, 4])\n",
      "loss 0 tensor(1.3211, grad_fn=<NllLossBackward>)\n",
      "meilleur loss: tensor(1.3211, grad_fn=<NllLossBackward>)\n",
      "shape conv 1 :  torch.Size([100, 128, 19683])\n",
      "shape resblock 1 :  torch.Size([100, 128, 2187])\n",
      "shape resblock 2 :  torch.Size([100, 256, 27])\n",
      "shape GRU :  torch.Size([100, 1024])\n",
      "shape output :  torch.Size([100, 4])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-ac57601be697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmeilleur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m99.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmeilleur_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-57914531e3b5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrawnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m                             \u001b[0;31m# Forward pass: compute the output class given a audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# Compute the loss: difference between the output class and the pre-given label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                   \u001b[0;31m# Backward pass: compute the weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                  \u001b[0;31m# Optimizer: update the weights of hidden nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "meilleur_loss = 99.\n",
    "for epoch in range(num_epochs):\n",
    "    train_model()\n",
    "    loss = cross_validation(rawnet, valid_loader)\n",
    "    if loss < meilleur_loss:\n",
    "        meilleur_loss = loss\n",
    "        print(\"meilleur loss:\", meilleur_loss)\n",
    "        #if (i+1) % num_epochs == 0:                              # Logging\n",
    "            #print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                    #%(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
