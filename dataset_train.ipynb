{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "#import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import Tensor\n",
    "from scipy.io import wavfile\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawNetDataset(Dataset):\n",
    "    def __init__(self, transform=None, mode=\"train\",files_dir=None, base_dir=\"\",csv_file_dir=\"\",nb_time=59049):\n",
    "        self.base_dir = base_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.csv_file_dir = csv_file_dir\n",
    "        self.files_dir = files_dir\n",
    "        csv_file = pd.read_csv(csv_file_dir,sep=\"\\t\")\n",
    "        self.nb_time = nb_time\n",
    "        #self.classes = {cls_name:i+1 for i, cls_name in enumerate(csv_file[\"VoxCeleb1 ID\"].unique())}\n",
    "        self.classes = {'id10009': 0,'id10016': 1,'id10017': 2,'id10019': 3}\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.files_dir)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.files_dir[idx]\n",
    "        classe = filename.split('/')[0]\n",
    "        X, sample_rate = torchaudio.load(self.base_dir + filename)\n",
    "        #print(\" shape(X): \",type(X))\n",
    "        label = self.classes[classe]\n",
    "        self._pre_emphasis(X)\n",
    "        nb_time = X.shape[1]\n",
    "        if nb_time > self.nb_time:\n",
    "            start_idx = np.random.randint(low = 0,\n",
    "                high = nb_time - self.nb_time)\n",
    "            X = X[:, start_idx:start_idx+self.nb_time]\n",
    "            #print(\"nb_time: \",nb_time )\n",
    "            #print(\"self.nb_time: \",self.nb_time)\n",
    "        elif nb_time < self.nb_time:\n",
    "            nb_dup = int(self.nb_time / nb_time) + 1\n",
    "            X = np.tile(X, (1, nb_dup))[:, :self.nb_time]\n",
    "            #print(\"taille inférieure\")\n",
    "        else:\n",
    "            X = X\n",
    "            #print(\"taille égale\")\n",
    "        #print(\" type(X): \",X.size())\n",
    "        return X, label\n",
    "\n",
    "    def _pre_emphasis(self, x):\n",
    "        '''\n",
    "        Pre-emphasis for single channel input\n",
    "        '''\n",
    "        return np.asarray(x[:,1:] - 0.97 * x[:, :-1], dtype=np.float32) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data/wav/'\n",
    "\n",
    "csv_file_dir = 'data/target/vox1_meta.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utt_list(src_dir):\n",
    "    l_utt = []\n",
    "    for r, ds, fs in os.walk(src_dir):\n",
    "        r = r.replace('\\\\', '/')   \n",
    "        base = '/'.join(r.split('/')[-2:])+'/'\n",
    "        for f in fs:\n",
    "            l_utt.append(base+f[:-4]+'.wav')\n",
    "    return l_utt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_IDs = get_utt_list(base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RawNetDataset(files_dir=list_IDs,base_dir=base_dir,csv_file_dir=csv_file_dir)\n",
    "batch_size = 128\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = RawNetDataset(files_dir=list_IDs,base_dir=base_dir,csv_file_dir=csv_file_dir)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6ff16862e332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mFeedforwardNeuralNetModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFeedforwardNeuralNetModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "\n",
    "        # Non-linearity\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function  # LINEAR\n",
    "        out = self.fc1(x)\n",
    "\n",
    "        # Non-linearity  # NON-LINEAR\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        # Linear function (readout)  # LINEAR\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiation du model\n",
    "input_dim = 59049\n",
    "hidden_dim = 100\n",
    "output_dim = 4\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation de la loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "#print(model.parameters())\n",
    "#print(len(list(model.parameters())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train model\n",
    "import numpy \n",
    "def train():\n",
    "    iter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, labels) in enumerate(train_loader):\n",
    "            # Load data with gradient accumulation capabilities\n",
    "            data = data.requires_grad_()\n",
    "\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass to get output/logits\n",
    "            outputs = model(data)\n",
    "            #print(\"-------------TYPE---------------\")\n",
    "            #print(outputs)\n",
    "\n",
    "            # Calculate Loss: softmax --> cross entropy loss\n",
    "            labels = labels.long()\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "            # Getting gradients w.r.t. parameters\n",
    "            #loss.backward()\n",
    "\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            iter += 1\n",
    "\n",
    "            if iter % 500 == 0:\n",
    "                # Calculate Accuracy         \n",
    "                correct = 0\n",
    "                total = 0\n",
    "                # Iterate through test dataset\n",
    "                for data, labels in test_loader:\n",
    "                    # Load data with gradient accumulation capabilities\n",
    "                    data = data.requires_grad_()\n",
    "\n",
    "                    # Forward pass only to get logits/output\n",
    "                    outputs = model(data)\n",
    "\n",
    "                    # Get predictions from the maximum value\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                    # Total number of labels\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                    # Total correct predictions\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "                accuracy = 100 * correct / total\n",
    "\n",
    "                # Print Loss\n",
    "                print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
